<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Second Try: Sentiment Analysis in Python | Andy Bromberg</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Second Try: Sentiment Analysis in Python" />
<meta name="author" content="Andy Bromberg" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A full walkthrough of performing sentiment analysis in Python." />
<meta property="og:description" content="A full walkthrough of performing sentiment analysis in Python." />
<link rel="canonical" href="https://andybromberg.com/sentiment-analysis-python" />
<meta property="og:url" content="https://andybromberg.com/sentiment-analysis-python" />
<meta property="og:site_name" content="Andy Bromberg" />
<meta property="og:image" content="https://andybromberg.com/assets/images/meta/posts/sentiment-analysis-python.png" />
<meta property="og:image:height" content="600" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:alt" content="Second Try: Sentiment Analysis in Python" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2013-02-14T13:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:image" content="https://andybromberg.com/assets/images/meta/posts/sentiment-analysis-python.png" />
<meta name="twitter:image:alt" content="Second Try: Sentiment Analysis in Python" />
<meta property="twitter:title" content="Second Try: Sentiment Analysis in Python" />
<meta name="twitter:site" content="@andy_bromberg" />
<meta name="twitter:creator" content="@Andy Bromberg" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Andy Bromberg"},"dateModified":"2013-02-14T13:00:00-06:00","datePublished":"2013-02-14T13:00:00-06:00","description":"A full walkthrough of performing sentiment analysis in Python.","headline":"Second Try: Sentiment Analysis in Python","image":{"width":1200,"height":600,"alt":"Second Try: Sentiment Analysis in Python","url":"https://andybromberg.com/assets/images/meta/posts/sentiment-analysis-python.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://andybromberg.com/sentiment-analysis-python"},"url":"https://andybromberg.com/sentiment-analysis-python"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate" type="application/rss+xml" href="/feed.xml" title="Andy Bromberg"/><link type="application/atom+xml" rel="alternate" href="https://andybromberg.com/feed.xml" title="Andy Bromberg" /><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JCTL6LCYXH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JCTL6LCYXH');
</script><svg width="0" height="0" style="position: absolute;">
    <defs>
      <clipPath id="highlighter-clip" clipPathUnits="objectBoundingBox">
        <path d="M0.02 0.1 Q0.02 0.02, 0.08 0.02 L0.92 0.01 Q0.99 0.01, 0.99 0.08 L0.98 0.92 Q0.98 0.99, 0.92 0.99 L0.08 0.98 Q0.01 0.98, 0.01 0.92 Z" />
      </clipPath>
    </defs>
  </svg>

</head>
<body><header class="" role="banner">

  <div class="wrapper"><a class="site-title bold" href="/">andy bromberg</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about">about</a><a class="page-link" href="/bookshelf">bookshelf</a><a class="page-link external-link" href="/newsletter">newsletter</a><a class="page-link rss-link" href="/feed.xml"><i class="fa-solid fa-square-rss"></i></a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Second Try: Sentiment Analysis in Python</h1>
    <div class="post-meta">

      <div class="post-date"><time class="dt-published" datetime="2013-02-14T13:00:00-06:00" itemprop="datePublished">February 14, 2013
        </time></div>
            
    <div class="article-reading-time">Reading time: about 9 minutes</div>
      
      
    </header>
    
    <div class="post-content e-content prose" itemprop="articleBody">
      <p class="dateline">Originally published <a href="https://web.archive.org/web/20190414054427/http://andybromberg.com/sentiment-analysis-python/" class="external-link" target="_blank" rel="noopener noreferrer">on my old blog</a> in 2013.</p>
<h2 id="introduction">
  
  
    Introduction <a href="#introduction" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p>After my <a href="/sentiment-analysis/">first experiments</a> with using R for sentiment analysis, I started talking with a friend here at school about my work. Jackson and I decided that we’d like to give it a better shot and really try to get some meaningful results. After a lot of research, we decided to shift languages to Python (even though we both know R). We made this shift because Python has a number of very useful libraries for text processing and sentiment analysis, plus it’s easy to code in. We launched right into tutorials and coding, and this post will be about that process and our results.</p>

<p>We also met with <a href="http://www.stanford.edu/~cgpotts/" class="external-link" target="_blank" rel="noopener noreferrer">Christopher Potts</a>, a professor of linguistics here at Stanford. Prior to meeting with him, we consulted <a href="http://sentiment.christopherpotts.net/" class="external-link" target="_blank" rel="noopener noreferrer">his sentiment analysis guide</a> extensively and found it incredibly useful. We had a fantastic chat with Professor Potts and he helped us grasp some of the concepts we were working on.</p>

<p>If you’d like to jump straight to seeing the full code, you can head over to the <a href="https://github.com/abromberg/sentiment_analysis_python" class="external-link" target="_blank" rel="noopener noreferrer">GitHub repository</a>.</p>
<h2 id="the-setup">
  
  
    The Setup <a href="#the-setup" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p>One of the resources we got a lot of mileage out of was <a href="http://streamhacker.com/" class="external-link" target="_blank" rel="noopener noreferrer">StreamHacker</a>, especially the articles on <a href="http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/" class="external-link" target="_blank" rel="noopener noreferrer">basic techniques</a>, <a href="http://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/" class="external-link" target="_blank" rel="noopener noreferrer">precision and recall</a> and <a href="http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/" class="external-link" target="_blank" rel="noopener noreferrer">eliminating features</a>. More to follow about each of those elements.</p>

<p>Another great discovery was the <a href="http://nltk.org/" class="external-link" target="_blank" rel="noopener noreferrer">Natural Language ToolKit</a> (NLTK). This is an incredible library for Python that can do a huge amount of text processing and analysis. This would end up forming the basis for our program.</p>

<p>During our first attempt, we basically just tried to convert my program in R into Python. We quickly realized that not only did Python have more efficient ways to do some of the steps, but it also was missing some functionality that I used in the R version. So instead, we started based on StreamHacker’s code.</p>

<p>An important piece of sentiment analysis terminology: “features” are whatever you’re analyzing in an attempt to correlate to the labels. For example, in this code, the features will be the words in each review. Other algorithms could use different types of features — some algorithms use bigrams or trigrams (strings of two or three consecutive words, respectively) as the features.</p>

<p>An idea from StreamHacker that we really liked was writing a function to evaluate different feature selection mechanisms. That means that we would be able to write different methods to select different subsets of the features (read: words) in the reviews and then evaluate those methods.</p>

<p>As an aside, here are the imports we used for the project, so I won’t have to reference them again:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">re</span><span class="p">,</span> <span class="n">math</span><span class="p">,</span> <span class="n">collections</span><span class="p">,</span> <span class="n">itertools</span>
<span class="kn">import</span> <span class="n">nltk</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">classify</span><span class="p">.</span><span class="n">util</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">metrics</span>
<span class="kn">from</span> <span class="n">nltk.classify</span> <span class="kn">import</span> <span class="n">NaiveBayesClassifier</span>
<span class="kn">from</span> <span class="n">nltk.metrics</span> <span class="kn">import</span> <span class="n">BigramAssocMeasures</span>
<span class="kn">from</span> <span class="n">nltk.probability</span> <span class="kn">import</span> <span class="n">FreqDist</span><span class="p">,</span> <span class="n">ConditionalFreqDist</span></code></pre></figure>

<p>We began to write our feature evaluator (again, many thanks to StreamHacker.):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">evaluate_features</span><span class="p">(</span><span class="n">feature_select</span><span class="p">):</span>
    <span class="c1">#reading pre-labeled input and splitting into lines
</span>    <span class="n">posSentences</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">polarityData</span><span class="se">\\</span><span class="s">rt-polaritydata</span><span class="se">\\</span><span class="s">rt-polarity-pos.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">negSentences</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">polarityData</span><span class="se">\\</span><span class="s">rt-polaritydata</span><span class="se">\\</span><span class="s">rt-polarity-neg.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">posSentences</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">posSentences</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
    <span class="n">negSentences</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">negSentences</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
    <span class="n">posFeatures</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">negFeatures</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1">#http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation
</span>    <span class="c1">#breaks up the sentences into lists of individual words (as selected by the input mechanism) and appends 'pos' or 'neg' after each list
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">posSentences</span><span class="p">:</span>
        <span class="n">posWords</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">[\w</span><span class="sh">'</span><span class="s">]+|[.,!?;]</span><span class="sh">"</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">posWords</span> <span class="o">=</span> <span class="p">[</span><span class="nf">feature_select</span><span class="p">(</span><span class="n">posWords</span><span class="p">),</span> <span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">posFeatures</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">posWords</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">negSentences</span><span class="p">:</span>
        <span class="n">negWords</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">[\w</span><span class="sh">'</span><span class="s">]+|[.,!?;]</span><span class="sh">"</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">negWords</span> <span class="o">=</span> <span class="p">[</span><span class="nf">feature_select</span><span class="p">(</span><span class="n">negWords</span><span class="p">),</span> <span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">negFeatures</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">negWords</span><span class="p">)</span></code></pre></figure>

<p>This is the same polarity data that was used in my <a href="/sentiment-analysis">previous post</a>, so check that out if you’re curious about the data. Essentially what that block of code does is splits up the reviews by line and then builds a posFeatures variable which contains the output of our feature selection mechanism (we’ll see how that works in a minute) with ‘pos’ or ‘neg’ appended to it, depending on whether the review it is drawing from is positive or negative.</p>

<p>The next bit of code separates the data into training and testing data for a Naive Bayes classifier, which is the same type of classifier I used before.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#selects 3/4 of the features to be used for training and 1/4 to be used for testing
</span>    <span class="n">posCutoff</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">posFeatures</span><span class="p">)</span><span class="o">*</span><span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">negCutoff</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">negFeatures</span><span class="p">)</span><span class="o">*</span><span class="mi">3</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">trainFeatures</span> <span class="o">=</span> <span class="n">posFeatures</span><span class="p">[:</span><span class="n">posCutoff</span><span class="p">]</span> <span class="o">+</span> <span class="n">negFeatures</span><span class="p">[:</span><span class="n">negCutoff</span><span class="p">]</span>
    <span class="n">testFeatures</span> <span class="o">=</span> <span class="n">posFeatures</span><span class="p">[</span><span class="n">posCutoff</span><span class="p">:]</span> <span class="o">+</span> <span class="n">negFeatures</span><span class="p">[</span><span class="n">negCutoff</span><span class="p">:]</span></code></pre></figure>

<p>Now, thanks to NLTK, I can very simply train my classifier:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">classifier</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NaiveBayesClassifier.train</span><span class="p">(</span><span class="n">trainFeatures</span><span class="p">)</span></code></pre></figure>

<p>Pretty cool, huh? The last thing this function needs to do is check how well the classifier does when it tries to classify the testing data. This code is a little challenging so I’ll walk through it thoroughly.</p>

<p>First, I have to initiate referenceSets and testSets, to be used shortly. referenceSets will contain the actual values for the testing data (which we know because the data is prelabeled) and testSets will contain the predicted output.</p>

<p>Next, for each one of the testFeatures (the reviews that need testing), I iterate through three things: an arbitrary ‘i’, so be used as an identifier, and then the features (or words) in the review, and the actual label (‘pos’ or ‘neg’).</p>

<p>I add the ‘i’ (the unique identifier) to the correct bin in referenceSets. I then predict the label based on the features using the trained classifier and put the unique identifier in the predicted bin in testSets.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">testFeatures</span><span class="p">):</span>
        <span class="n">referenceSets</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="nf">add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">classify</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">testSets</span><span class="p">[</span><span class="n">predicted</span><span class="p">].</span><span class="nf">add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span></code></pre></figure>

<p>This gives me a big list of identifiers in referenceSets[‘pos’], which are the reviews known to be positive (and the same for the negative reviews). It also gives me a list of identifiers in testSets[‘pos’], which are the reviews predicted to be positive (and similarly for predicted negatives). What this allows me to do is to compare these lists and see how well the predictor did. Here’s where one of the StreamHacker articles (on <a href="https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/" class="external-link" target="_blank" rel="noopener noreferrer">precision and recall</a>) really helped.</p>

<p>The essence of those two terms is that precision is a measure of false positives — a higher precision means fewer reviews that aren’t in the desired label get labeled as being in there. A high recall means fewer reviews that are in the desired label get put in the wrong level. As you can imagine, these metrics correlate very closely. Here’s the code (again from the NLTK library) to print out the positive and negative recall and precision, as well as the accuracy (a less-specific measure just showing what percentage the classifier got right). NLTK also has a cool function that shows the features (words) that were most helpful to the classifier in determining whether a review was positive or negative. So here’s the code:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span> <span class="sh">'</span><span class="s">train on %d instances, test on %d instances</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">trainFeatures</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">testFeatures</span><span class="p">))</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">accuracy:</span><span class="sh">'</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">classify</span><span class="p">.</span><span class="n">util</span><span class="p">.</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">testFeatures</span><span class="p">)</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">pos precision:</span><span class="sh">'</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">precision</span><span class="p">(</span><span class="n">referenceSets</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">],</span> <span class="n">testSets</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">pos recall:</span><span class="sh">'</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">recall</span><span class="p">(</span><span class="n">referenceSets</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">],</span> <span class="n">testSets</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">neg precision:</span><span class="sh">'</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">precision</span><span class="p">(</span><span class="n">referenceSets</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">],</span> <span class="n">testSets</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">neg recall:</span><span class="sh">'</span><span class="p">,</span> <span class="n">nltk</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">recall</span><span class="p">(</span><span class="n">referenceSets</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">],</span> <span class="n">testSets</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">classifier</span><span class="p">.</span><span class="nf">show_most_informative_features</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span></code></pre></figure>
<h2 id="the-basic-method">
  
  
    The Basic Method <a href="#the-basic-method" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p>So after all that, we can start to figure out our feature selection mechanism. This basically means the way we select which words to train the classifier on.</p>

<p>In my previous post, I didn’t actually train the classifier on words at all. The classifier was trained on the number of words in each category from very negative to very positive. With this Python program, Jackson and I chose to look at the individual words themselves rather than counting positive and negative words.</p>

<p>We did this because there is inherent error in picking positive and negative words — there’s a huge loss of information there: sentence-long reviews were reduced down to just a few digits. With this method, we’re keeping a lot more of the information in the review.<label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle" /><span class="sidenote">Although we certainly don’t keep all the information in the review, since sentence structure and punctuation disappear and we just look at words in a vacuum. Improving that might be a direction for us to go in the future.</span></p>

<p>The most obvious feature selection mechanism is just to look at all the words in each review — it’s simple to code and provides a great base case. Here’s all we need:</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">def</span><span class="w"> </span><span class="n">make_full_dict</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">:</span><span class="w">
    </span><span class="n">return</span><span class="w"> </span><span class="n">dict</span><span class="p">([(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">True</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">words</span><span class="p">])</span></code></pre></figure>

<p>This just builds a <a href="http://docs.python.org/2/tutorial/datastructures.html#dictionaries" class="external-link" target="_blank" rel="noopener noreferrer">dictionary object</a> (what we need for the evaluate_features method) that has each of the words in the review followed by ‘True’.</p>

<p>Then we can just run the testing method:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">using</span> <span class="nb">all</span> <span class="n">words</span> <span class="k">as</span> <span class="n">features</span>
<span class="n">train</span> <span class="n">on</span> <span class="mi">7998</span> <span class="n">instances</span><span class="p">,</span> <span class="n">test</span> <span class="n">on</span> <span class="mi">2666</span> <span class="n">instances</span>
<span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.773068267067</span>
<span class="n">pos</span> <span class="n">precision</span><span class="p">:</span> <span class="mf">0.787066246057</span>
<span class="n">pos</span> <span class="n">recall</span><span class="p">:</span> <span class="mf">0.748687171793</span>
<span class="n">neg</span> <span class="n">precision</span><span class="p">:</span> <span class="mf">0.760371959943</span>
<span class="n">neg</span> <span class="n">recall</span><span class="p">:</span> <span class="mf">0.797449362341</span>
<span class="n">Most</span> <span class="n">Informative</span> <span class="n">Features</span>
              <span class="n">engrossing</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">17.0</span> <span class="p">:</span> <span class="mf">1.0</span>
                   <span class="n">quiet</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">15.7</span> <span class="p">:</span> <span class="mf">1.0</span>
                <span class="n">mediocre</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">neg</span> <span class="p">:</span> <span class="n">pos</span>    <span class="o">=</span>     <span class="mf">13.7</span> <span class="p">:</span> <span class="mf">1.0</span>
               <span class="n">absorbing</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">13.0</span> <span class="p">:</span> <span class="mf">1.0</span>
                <span class="n">portrait</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">12.4</span> <span class="p">:</span> <span class="mf">1.0</span>
              <span class="n">refreshing</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">12.3</span> <span class="p">:</span> <span class="mf">1.0</span>
                   <span class="n">flaws</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">12.3</span> <span class="p">:</span> <span class="mf">1.0</span>
               <span class="n">inventive</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">12.3</span> <span class="p">:</span> <span class="mf">1.0</span>
                 <span class="n">triumph</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">11.7</span> <span class="p">:</span> <span class="mf">1.0</span>
            <span class="n">refreshingly</span> <span class="o">=</span> <span class="bp">True</span>              <span class="n">pos</span> <span class="p">:</span> <span class="n">neg</span>    <span class="o">=</span>     <span class="mf">11.7</span> <span class="p">:</span> <span class="mf">1.0</span></code></pre></figure>

<p>That might seem overwhelming, but we can go through it bit by bit!</p>

<p>First, we see that the accuracy is 77% — this is already seeming a lot better than my first attempt with R. Then we see that the precisions and recalls are all pretty close to each other, which means that it is classifying everything fairly evenly. No problems there, and we don’t need to read a whole lot more into it.</p>

<p>Then we see the most informative features — for example, if ‘engrossing’ is in a review, there’s a 17:1 chance the review is positive.</p>

<p>As a side note, there’s a couple interesting ones in there. For example, ‘flaws’ being in a review strongly indicates that the review is positive. Perhaps that’s because people rarely use “flaws” in a very negative sense, typically opting for stronger words. However, it is common to hear “it had some flaws, but…”</p>
<h2 id="the-next-step">
  
  
    The Next Step <a href="#the-next-step" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p>Moving on, the next way to select features is to only take the n most informative features — basically, the features that convey the most information. Again (for the millionth time), thanks to StreamHacker here.</p>

<p>We first need to find the information gain of each word. This is a big chunk of code, but we’ll break it up.</p>

<p>First, we broke up the words in a similar way to in the evaluate_features function and made them iterable (so that we could iterate through them):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">create_word_scores</span><span class="p">():</span>
    <span class="c1">#splits sentences into lines
</span>    <span class="n">posSentences</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">polarityData</span><span class="se">\\</span><span class="s">rt-polaritydata</span><span class="se">\\</span><span class="s">rt-polarity-pos.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">negSentences</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">polarityData</span><span class="se">\\</span><span class="s">rt-polaritydata</span><span class="se">\\</span><span class="s">rt-polarity-neg.txt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">posSentences</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">posSentences</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
    <span class="n">negSentences</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\n</span><span class="sh">'</span><span class="p">,</span> <span class="n">negSentences</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
 
    <span class="c1">#creates lists of all positive and negative words
</span>    <span class="n">posWords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">negWords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">posSentences</span><span class="p">:</span>
        <span class="n">posWord</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">[\w</span><span class="sh">'</span><span class="s">]+|[.,!?;]</span><span class="sh">"</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">posWords</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">posWord</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">negSentences</span><span class="p">:</span>
        <span class="n">negWord</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">findall</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">[\w</span><span class="sh">'</span><span class="s">]+|[.,!?;]</span><span class="sh">"</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">negWords</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">negWord</span><span class="p">)</span>
    <span class="n">posWords</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">itertools</span><span class="p">.</span><span class="nf">chain</span><span class="p">(</span><span class="o">*</span><span class="n">posWords</span><span class="p">))</span>
    <span class="n">negWords</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">itertools</span><span class="p">.</span><span class="nf">chain</span><span class="p">(</span><span class="o">*</span><span class="n">negWords</span><span class="p">))</span></code></pre></figure>

<p>Then we set up an overall frequency distribution of all the words, which can be visualized as a huge histogram with the number of each word in all the reviews combined. However, with just this line, all we do is initialize the frequency distribution — it’s actually empty:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">word_fd</span> <span class="o">=</span> <span class="nc">FreqDist</span><span class="p">()</span></code></pre></figure>

<p>We’ll also need a conditional frequency distribution — a distribution that takes into account whether the word is in a positive or negative review. This can be visualized as two different histograms, one with all the words in positive reviews, and one with all the words in negative reviews. Like above, this is just an empty conditional frequency distribution. Nothing is in there yet.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">cond_word_fd</span> <span class="o">=</span> <span class="nc">ConditionalFreqDist</span><span class="p">()</span></code></pre></figure>

<p>Then, we essentially fill out the frequency distributions, incrementing (with .inc) the counter of each word within the appropriate distribution.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">posWords</span><span class="p">:</span>
        <span class="n">word_fd</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">].</span><span class="nf">inc</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">negWords</span><span class="p">:</span>
        <span class="n">word_fd</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>
        <span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">].</span><span class="nf">inc</span><span class="p">(</span><span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span></code></pre></figure>

<p>The next thing we need to find the highest-information features is the count of words in positive reviews, words in negative reviews, and total words:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">pos_word_count</span> <span class="o">=</span> <span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">].</span><span class="nc">N</span><span class="p">()</span>
    <span class="n">neg_word_count</span> <span class="o">=</span> <span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">].</span><span class="nc">N</span><span class="p">()</span>
    <span class="n">total_word_count</span> <span class="o">=</span> <span class="n">pos_word_count</span> <span class="o">+</span> <span class="n">neg_word_count</span></code></pre></figure>

<p>The last thing we need to do is use a <a href="http://en.wikipedia.org/wiki/Chi-squared_test" class="external-link" target="_blank" rel="noopener noreferrer">chi-squared test</a> test (also from NLTK) to score the words. We find each word’s positive information score and negative information score, add them up, and fill up a dictionary correlating the words and scores, which we then return out of the function. Chi-squared tests, as you can read in the Wikipedia article I just linked to, is a great way to see how much information a given input conveys.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">word_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_fd</span><span class="p">.</span><span class="nf">iteritems</span><span class="p">():</span>
        <span class="n">pos_score</span> <span class="o">=</span> <span class="n">BigramAssocMeasures</span><span class="p">.</span><span class="nf">chi_sq</span><span class="p">(</span><span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">pos</span><span class="sh">'</span><span class="p">][</span><span class="n">word</span><span class="p">],</span> <span class="p">(</span><span class="n">freq</span><span class="p">,</span> <span class="n">pos_word_count</span><span class="p">),</span> <span class="n">total_word_count</span><span class="p">)</span>
        <span class="n">neg_score</span> <span class="o">=</span> <span class="n">BigramAssocMeasures</span><span class="p">.</span><span class="nf">chi_sq</span><span class="p">(</span><span class="n">cond_word_fd</span><span class="p">[</span><span class="sh">'</span><span class="s">neg</span><span class="sh">'</span><span class="p">][</span><span class="n">word</span><span class="p">],</span> <span class="p">(</span><span class="n">freq</span><span class="p">,</span> <span class="n">neg_word_count</span><span class="p">),</span> <span class="n">total_word_count</span><span class="p">)</span>
        <span class="n">word_scores</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos_score</span> <span class="o">+</span> <span class="n">neg_score</span>
 
    <span class="k">return</span> <span class="n">word_scores</span></code></pre></figure>

<p>We then make another function that finds the best <em>n</em> words, given a set of scores (which we’ll calculate using the function we just made) and an <em>n</em>:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">find_best_words</span><span class="p">(</span><span class="n">word_scores</span><span class="p">,</span> <span class="n">number</span><span class="p">):</span>
    <span class="n">best_vals</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">word_scores</span><span class="p">.</span><span class="nf">iteritems</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="nf">lambda </span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span> <span class="n">s</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">number</span><span class="p">]</span>
    <span class="n">best_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">best_vals</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">best_words</span></code></pre></figure>

<p>Are you a little confused by that ‘lambda’? I certainly was when I saw it. Essentially what it does is allow you to temporarily make a function to return something. In this case, it’s helping to sort the words into the correct order. You can check <a href="http://www.secnetix.de/olli/Python/lambda_functions.hawk" class="external-link" target="_blank" rel="noopener noreferrer">this</a> out for more information on lambda functions.</p>

<p>Finally, we can make a feature selection mechanism that returns ‘True’ for a word only if it is in the best words list:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">best_word_features</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">dict</span><span class="p">([(</span><span class="n">word</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">best_words</span><span class="p">])</span></code></pre></figure>

<p>Last, I ran it using the best 10, 100, 1000, 10000, and 15000 words. Here’s the code to do that:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">numbers_to_test</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">15000</span><span class="p">]</span>
<span class="c1">#tries the best_word_features mechanism with each of the numbers_to_test of features
</span><span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">numbers_to_test</span><span class="p">:</span>
    <span class="k">print</span> <span class="sh">'</span><span class="s">evaluating best %d word features</span><span class="sh">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="n">best_words</span> <span class="o">=</span> <span class="nf">find_best_words</span><span class="p">(</span><span class="n">word_scores</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="nf">evaluate_features</span><span class="p">(</span><span class="n">best_word_features</span><span class="p">)</span></code></pre></figure>

<p>Here’s my output (I’ve cut out the informative features list because it’s the same for all of them, including the one using all the features):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>evaluating best 10 word features
train on 7998 instances, test on 2666 instances
accuracy: 0.574643660915
pos precision: 0.549379652605
pos recall: 0.830457614404
neg precision: 0.652841781874
neg recall: 0.318829707427

evaluating best 100 word features
train on 7998 instances, test on 2666 instances
accuracy: 0.682295573893
pos precision: 0.659868421053
pos recall: 0.752438109527
neg precision: 0.712041884817
neg recall: 0.61215303826

evaluating best 1000 word features
train on 7998 instances, test on 2666 instances
accuracy: 0.79632408102
pos precision: 0.817014446228
pos recall: 0.763690922731
neg precision: 0.778169014085
neg recall: 0.82895723931

evaluating best 10000 word features
train on 7998 instances, test on 2666 instances
accuracy: 0.846586646662
pos precision: 0.868421052632
pos recall: 0.81695423856
neg precision: 0.827195467422
neg recall: 0.876219054764

evaluating best 15000 word features
train on 7998 instances, test on 2666 instances
accuracy: 0.846961740435
pos precision: 0.862745098039
pos recall: 0.825206301575
neg precision: 0.832494608196
neg recall: 0.868717179295
</code></pre></div></div>
<h2 id="the-conclusions">
  
  
    The Conclusions <a href="#the-conclusions" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p>There’s lots to read into here. Obviously, using very few features didn’t do great in terms of accuracy, precision, and recall because there wasn’t enough data to build the model off of. Using 1000 features is about as good as using all the features, but at 10000 and 15000, there’s a pretty huge increase over the base case, getting up to ~85% accuracy and similar precision and recall statistics.</p>

<p>That means that using intelligent feature selection increased the accuracy by around 8 percentage points, which seems like quite a significant jump. Jackson and I were very happy about that.</p>

<p>We’re also happy about the results as a whole — classifying reviews with over 80% accuracy is pretty impressive and we can see lots of applications for this technology.</p>

<p>Of course, there are tons of ways to improve these results. These include (but are not limited to):</p>

<ol>
  <li>Adding different feature selection mechanisms</li>
  <li>Pre-processing the text to get rid of unimportant words or punctuation</li>
  <li>Doing deeper analysis of the sentences as a whole</li>
  <li>Trying a different classifier than the Naive Bayes Classifier</li>
</ol>

<p>A disclaimer applies: we’re just learning all of this, and fairly independently too. There’s a decent chance that there’s a mistake or an inappropriate conclusion somewhere. If there is, please don’t hesitate to <a href="mailto:andy@andybromberg.com" class="mailto-link">email me</a>. I’d also love to hear from you if you have any other input on the project!</p>

<p>For the full code, check out the <a href="https://github.com/abromberg/sentiment_analysis_python" class="external-link" target="_blank" rel="noopener noreferrer">GitHub repository</a>.</p>
<h2 id="addendum">
  
  
    Addendum <a href="#addendum" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<p><a href="http://protobi.com/" class="external-link" target="_blank" rel="noopener noreferrer">Pieter Sheth-Voss</a> pointed out in the comments section that there’s a linear relationship between the accuracy and the log of the number of features. I decided to take a look at this in R. Here’s the code I used.</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">250</span><span class="p">,</span><span class="w"> </span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="m">750</span><span class="p">,</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w"> </span><span class="m">2500</span><span class="p">,</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w"> </span><span class="m">7500</span><span class="p">,</span><span class="w"> </span><span class="m">10000</span><span class="p">,</span><span class="w"> </span><span class="m">12500</span><span class="p">,</span><span class="w"> </span><span class="m">15000</span><span class="p">)</span><span class="w">
</span><span class="n">logFeatures</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="w">
</span><span class="n">accuracy</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.5746436609152288</span><span class="p">,</span><span class="w"> </span><span class="m">0.6406601650412603</span><span class="p">,</span><span class="w"> </span><span class="m">0.6822955738934734</span><span class="p">,</span><span class="w"> </span><span class="m">0.7291822955738935</span><span class="p">,</span><span class="w"> </span><span class="m">0.7659414853713429</span><span class="p">,</span><span class="w"> </span><span class="m">0.7861965491372843</span><span class="p">,</span><span class="w"> </span><span class="m">0.7963240810202551</span><span class="p">,</span><span class="w"> </span><span class="m">0.8304576144036009</span><span class="p">,</span><span class="w"> </span><span class="m">0.8510877719429858</span><span class="p">,</span><span class="w"> </span><span class="m">0.8465866466616654</span><span class="p">,</span><span class="w"> </span><span class="m">0.8465866466616654</span><span class="p">,</span><span class="w"> </span><span class="m">0.8465866466616654</span><span class="p">,</span><span class="w"> </span><span class="m">0.8469617404351087</span><span class="p">)</span><span class="w">
</span><span class="n">data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">logFeatures</span><span class="p">,</span><span class="w"> </span><span class="n">accuracy</span><span class="p">)</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">logFeatures</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="n">accuracy</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="n">lm</span><span class="p">)</span></code></pre></figure>

<p>This takes advantage of the great <a href="http://ggplot2.org/" class="external-link" target="_blank" rel="noopener noreferrer">ggplot2</a> library for R. It simply puts in the features, calculates the log of them, puts in the accuracy, and plots these two variables. Here’s the output:</p>

<picture loading="lazy"><source srcset="/assets/images/generated/sentiment-analysis-python/features_performance_all-400-d24067f35.webp 400w, /assets/images/generated/sentiment-analysis-python/features_performance_all-600-d24067f35.webp 600w" type="image/webp" /><source srcset="/assets/images/generated/sentiment-analysis-python/features_performance_all-400-06e292092.png 400w, /assets/images/generated/sentiment-analysis-python/features_performance_all-600-06e292092.png 600w" type="image/png" /><img src="/assets/images/generated/sentiment-analysis-python/features_performance_all-600-06e292092.png" width="600" height="500" /></picture>

<p>I noticed here at the last 4 values seem a bit out of line and ran the script again with those removed to see how well the relationship performed through 5000 features:</p>

<picture loading="lazy"><source srcset="/assets/images/generated/sentiment-analysis-python/features_performance_partial-400-0f8722443.webp 400w, /assets/images/generated/sentiment-analysis-python/features_performance_partial-600-0f8722443.webp 600w" type="image/webp" /><source srcset="/assets/images/generated/sentiment-analysis-python/features_performance_partial-400-79f91b3a7.png 400w, /assets/images/generated/sentiment-analysis-python/features_performance_partial-600-79f91b3a7.png 600w" type="image/png" /><img src="/assets/images/generated/sentiment-analysis-python/features_performance_partial-600-79f91b3a7.png" width="600" height="500" /></picture>

<p>This is quite a strong correlation. Very interesting how well that worked out. Thanks for the heads up, Pieter!</p><hr class="end-of-article-divider" />
<h2 class="no_toc">
  
  
    Looking for more to read? <a href="#addendum" class="anchor-link"><i class="fas fa-link"></i></a>
  
  
</h2>
    

<nav class="post-navigation">
  <div class="prev-post">
    
    <a href="/sentiment-analysis">
    
    <div class="post-nav-header"><i class="fa-solid fa-arrow-left post-nav-arrow"></i> Previous post</div>
    
    </a>
    
    
      <a href="/sentiment-analysis">First shot: Sentiment Analysis in R</a>
    
  </div>
  <div class="next-post">
    <div class="post-nav-header">
        
        <a href="/credit-cards">
        
        Next post <i class="fa-solid fa-arrow-right post-nav-arrow"></i>
        
        </a>
        
    </div>
    
      <a href="/credit-cards">Weekend Project: Hacking the Square Reader</a>
    
  </div>
</nav>

<div>
    <p>Want to hear about new essays? Subscribe to my roughly-monthly <a href="https://andybromberg.substack.com" class="external-link" target="_blank" rel="noopener noreferrer">newsletter</a> recapping my recent writing and things I'm enjoying:</p>
    <div class="substack">
        <iframe loading="lazy" src="https://andybromberg.substack.com/embed" width="480" height="150" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
    </div>

    <p>And I'd love to hear from you directly: <a href="mailto:andy@andybromberg.com" class="mailto-link">andy@andybromberg.com</a></p>
</div>

    </div><a class="u-url" href="/sentiment-analysis-python" hidden></a>
  </article>
  
      </div>
    </main></body>

</html>
